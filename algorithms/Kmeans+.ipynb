{"cells":[{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17707,"status":"ok","timestamp":1668445295774,"user":{"displayName":"孙菲阳","userId":"00080526783451141965"},"user_tz":-60},"id":"WAi8dgc-J5Jn","outputId":"30dfbcaf-271f-49d4-e455-959ebdbae8f0"},"outputs":[],"source":["%matplotlib inline\n","# from ExKMC.Tree import Tree # import from cloned local library followed by installing manually\n","import sys\n","sys.path.append('../')\n","from ExKMC_M.ExKMC.Tree import Tree\n","\n","from sklearn.datasets import make_blobs\n","import gdown\n","import pandas as pd\n","import copy\n","from sklearn.cluster import KMeans\n","from utils import calc_cost, plot_kmeans, plot_tree_boundary,plot_confusion_matrix\n","from sklearn.preprocessing import StandardScaler, normalize\n","from utils import plot_confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import os\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# print(os.path.join(os.path.dirname(__file__),\"./data/negtive.csv\"))\n","# print(os.path.join(os.path.abspath('algorithms/Kmeans+.ipynb'),\"./data/negtive.csv\"))\n","# print(os.path.abspath('algorithms/Kmeans+.ipynb'))"]},{"cell_type":"markdown","metadata":{},"source":["## data preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# input data\n","def getDataDrive(url, output, isImport=False):\n","    \"\"\"\n","    return pandas dataframe\n","    \"\"\"\n","    if isImport:\n","        gdown.download(url=url, output=output, quiet=False)\n","    res = pd.read_csv(output)\n","    return res\n","\n","\n","neg = getDataDrive(\n","    url=\"https://drive.google.com/uc?id=1ocidTn7jUvCrLG_XJ6H9MiNUDexCkjFG\",\n","    output='/home/sfy/Documents/VScodeProject/Thesis/data/negtive.csv'\n",")\n","pos = getDataDrive(\n","    url=\"https://drive.google.com/uc?id=1IyMPjACBkz96giGJ-Z4IMk-qzM-1CJ9G\",\n","    output='/home/sfy/Documents/VScodeProject/Thesis/data/positive.csv',\n",")\n","\n","X_neg = copy.deepcopy(neg)\n","X_pos = copy.deepcopy(pos)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["pos_target = [1 for _ in range(X_pos.shape[0])]\n","neg_target = [0 for _ in range(X_neg.shape[0])]\n","\n","X_pos['y'] = pos_target\n","X_neg['y'] = neg_target\n","\n","X__ = pd.concat([X_pos,X_neg])\n","\n","# exclue name -> X_\n","X_ = X__.loc[:, X__.columns!='name']\n","\n","# exclude label -> X\n","y = X_['y']\n","X = X_.loc[:, X_.columns!='y']\n","\n","# data preprocess\n","# Standardize data\n","# TODO train_test_split(X, y, test_size=0.4, random_state=0)\n","\n","scaler = StandardScaler() \n","scaled_df = scaler.fit_transform(X) \n","  \n","# Normalizing the Data \n","normalized_df = normalize(scaled_df) \n","  \n","# Converting the numpy array into a pandas DataFrame \n","X = pd.DataFrame(normalized_df) \n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["(41257, 696)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["X.shape"]},{"cell_type":"markdown","metadata":{},"source":["## ExKMC algorithm"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# kmeans pre train\n","k = 2\n","n = X.shape[0]\n","\n","\n","kmeans = KMeans(k, random_state=42)\n","kmeans.fit(X)\n","\n","# confusion_matrix\n","# plot_confusion_matrix(y, kmeans.predict(X), np.array(list(X_.columns)), normalize=True)\n","\n","def visualize_2d(data):\n","    from sklearn.decomposition import PCA\n","    pca = PCA(2)\n","    df = pca.fit_transform(data.to_numpy())\n","\n","    kmeans_2 = KMeans(k, random_state=42)\n","    kmeans_2.fit(df)\n","\n","    plot_kmeans(kmeans_2, x_data =df)\n","\n","    tree_n = Tree(k)\n","    tree_n.fit(df, kmeans_2)\n","\n","    plot_tree_boundary(tree_n, k, df, kmeans_2, plot_mistakes=True)\n","\n","# visualize_2d(X)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["cluster_labels = kmeans.fit_predict(X)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["array([[-0.00569809, -0.0144497 , -0.01241951, ...,  0.        ,\n","         0.        ,  0.        ],\n","       [-0.03171698, -0.01341128, -0.00573732, ...,  0.        ,\n","         0.        ,  0.        ]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["kmeans.cluster_centers_"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["13"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["kmeans.n_iter_"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# test with inter \n","def inter_visual():\n","    from yellowbrick.cluster import InterclusterDistance\n","    visualizer = InterclusterDistance(kmeans)\n","    visualizer.fit(X)\n","    visualizer.show()\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["<ExKMC_M.ExKMC.Tree.Tree at 0x7f84b62bbd30>"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize tree with up to 6 leaves, predicting 3 clusters\n","tree = Tree(k=k)\n","\n","# Construct the tree, and return cluster labels\n","# prediction = tree.fit_predict(X,kmeans)\n","tree.fit(X, kmeans)\n","\n","# Tree plot saved to filename\n","# tree.plot(\"test\",feature_names=list(X_.columns))"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["tree_labels = tree.fit_predict(X,kmeans)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 1.])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["tree_labels"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["kmeans_cost is 37179.87612739552 \n","surrogate_score is 37290.76841843778\n","kmenas cost in surrogate is -36683.55002984239\n"]}],"source":["# cost\n","# kmeas cost in paper: The k-means cost is the sum of squared distances of each point to the mean of points associated with the cluster.\n","# kmenas cost in sklearn:Opposite of the value of X on the K-means objective.\n","# surrogate cost:The k-means surrogate cost is the sum of squared distances of each point to the closest center of the kmeans given (or trained) in the fit method.k-means surrogate cost > k-means cost, as k-means cost is computed with respect to the optimal centers.\n","\n","kmeas_cost = tree.score(X)\n","surrogate_score = tree.surrogate_score(X)\n","print(f\"kmeans_cost is {kmeas_cost} \\nsurrogate_score is {surrogate_score}\\nkmenas cost in surrogate is {kmeans.score(X)}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### inter distance"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inter distance from kmeans:0.5800166565176593\n","inter distance from tree:0.5800166565176593\n"]}],"source":["# inter-distance\n","from scipy.spatial.distance import euclidean\n","\n","dst = euclidean(kmeans.cluster_centers_[0], kmeans.cluster_centers_[1])\n","print(f'inter distance from kmeans:{dst}')\n","\n","tree_centers = tree.get_centers()\n","tree_dst = euclidean(tree_centers[0], tree_centers[1])\n","print(f'inter distance from tree:{tree_dst}')\n"]},{"cell_type":"markdown","metadata":{},"source":["### intra distance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.utils import check_random_state, check_X_y,_safe_indexing\n","from sklearn.metrics.pairwise import pairwise_distances,pairwise_distances_chunked\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","def silhouette_score(\n","    X, labels, *, metric=\"euclidean\", sample_size=None, random_state=None, **kwds\n","):\n","    if sample_size is not None:\n","        X, labels = check_X_y(X, labels, accept_sparse=[\"csc\", \"csr\"])\n","        random_state = check_random_state(random_state)\n","        indices = random_state.permutation(X.shape[0])[:sample_size]\n","        if metric == \"precomputed\":\n","            X, labels = X[indices].T[indices].T, labels[indices]\n","        else:\n","            X, labels = X[indices], labels[indices]\n","    return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))\n","\n","def silhouette_samples(X, labels, *, metric=\"euclidean\", **kwds):\n","    \"\"\"Compute the Silhouette Coefficient for each sample.\n","\n","    The Silhouette Coefficient is a measure of how well samples are clustered\n","    with samples that are similar to themselves. Clustering models with a high\n","    Silhouette Coefficient are said to be dense, where samples in the same\n","    cluster are similar to each other, and well separated, where samples in\n","    different clusters are not very similar to each other.\n","\n","    The Silhouette Coefficient is calculated using the mean intra-cluster\n","    distance (``a``) and the mean nearest-cluster distance (``b``) for each\n","    sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n","    b)``.\n","    Note that Silhouette Coefficient is only defined if number of labels\n","    is 2 ``<= n_labels <= n_samples - 1``.\n","\n","    This function returns the Silhouette Coefficient for each sample.\n","\n","    The best value is 1 and the worst value is -1. Values near 0 indicate\n","    overlapping clusters.\n","\n","    Read more in the :ref:`User Guide <silhouette_coefficient>`.\n","\n","    Parameters\n","    ----------\n","    X : array-like of shape (n_samples_a, n_samples_a) if metric == \\\n","            \"precomputed\" or (n_samples_a, n_features) otherwise\n","        An array of pairwise distances between samples, or a feature array.\n","\n","    labels : array-like of shape (n_samples,)\n","        Label values for each sample.\n","\n","    metric : str or callable, default='euclidean'\n","        The metric to use when calculating distance between instances in a\n","        feature array. If metric is a string, it must be one of the options\n","        allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n","        If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n","        Precomputed distance matrices must have 0 along the diagonal.\n","\n","    **kwds : optional keyword parameters\n","        Any further parameters are passed directly to the distance function.\n","        If using a ``scipy.spatial.distance`` metric, the parameters are still\n","        metric dependent. See the scipy docs for usage examples.\n","\n","    Returns\n","    -------\n","    silhouette : array-like of shape (n_samples,)\n","        Silhouette Coefficients for each sample.\n","    \"\"\"\n","    X, labels = check_X_y(X, labels, accept_sparse=[\"csc\", \"csr\"])\n","\n","    # Check for non-zero diagonal entries in precomputed distance matrix\n","    if metric == \"precomputed\":\n","        error_msg = ValueError(\n","            \"The precomputed distance matrix contains non-zero \"\n","            \"elements on the diagonal. Use np.fill_diagonal(X, 0).\"\n","        )\n","        if X.dtype.kind == \"f\":\n","            atol = np.finfo(X.dtype).eps * 100\n","            if np.any(np.abs(np.diagonal(X)) > atol):\n","                raise ValueError(error_msg)\n","        elif np.any(np.diagonal(X) != 0):  # integral dtype\n","            raise ValueError(error_msg)\n","\n","    le = LabelEncoder()\n","    labels = le.fit_transform(labels)\n","    n_samples = len(labels)\n","    label_freqs = np.bincount(labels)\n","    check_number_of_labels(len(le.classes_), n_samples)\n","\n","    kwds[\"metric\"] = metric\n","    reduce_func = functools.partial(\n","        _silhouette_reduce, labels=labels, label_freqs=label_freqs\n","    )\n","    results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func, **kwds))\n","    intra_clust_dists, inter_clust_dists = results\n","    intra_clust_dists = np.concatenate(intra_clust_dists)\n","    inter_clust_dists = np.concatenate(inter_clust_dists)\n","\n","    denom = (label_freqs - 1).take(labels, mode=\"clip\")\n","    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","        intra_clust_dists /= denom\n","\n","    sil_samples = inter_clust_dists - intra_clust_dists\n","    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","        sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)\n","    # nan values are for clusters of size 1, and should be 0\n","    return np.nan_to_num(sil_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### other useful build-in matrics (needs to implement one attribute)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from time import time\n","from sklearn import metrics\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","def bench_k_means(kmeans, name, data, labels):\n","    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n","\n","    Parameters\n","    ----------\n","    kmeans : KMeans instance\n","        A :class:`~sklearn.cluster.KMeans` instance with the initialization\n","        already set.\n","    name : str\n","        Name given to the strategy. It will be used to show the results in a\n","        table.\n","    data : ndarray of shape (n_samples, n_features)\n","        The data to cluster.\n","    labels : ndarray of shape (n_samples,)\n","        The labels used to compute the clustering metrics which requires some\n","        supervision.\n","    \"\"\"\n","    t0 = time()\n","    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n","    fit_time = time() - t0\n","    results = [name, fit_time, estimator[-1].inertia_]\n","\n","    # Define the metrics which require only the true labels and estimator\n","    # labels\n","    clustering_metrics = [\n","        metrics.homogeneity_score,\n","        metrics.completeness_score,\n","        metrics.v_measure_score,\n","        metrics.adjusted_rand_score,\n","        metrics.adjusted_mutual_info_score,\n","    ]\n","    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n","\n","    # The silhouette score requires the full dataset\n","    results += [\n","        metrics.silhouette_score(\n","            data,\n","            estimator[-1].labels_,\n","            metric=\"euclidean\",\n","            sample_size=300,\n","        )\n","    ]\n","\n","    # Show the results\n","    formatter_result = (\n","        \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n","    )\n","    print(formatter_result.format(*results))\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["kmeans   \t7.233s\t16612858\t0.017\t0.017\t0.017\t0.024\t0.017\t0.103\n"]}],"source":["bench_k_means(kmeans=kmeans,name='kmeans',data=X,labels=y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bench_k_means(kmeans=tree,name='tree',data=X,labels=y)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNrV8z86W1ElNSu4E21X3YW","provenance":[]},"kernelspec":{"display_name":"Python 3.8.15 ('thesis8')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"518c84c64c8714e835d71f45ff9dbbd66c2334f2195057bfe969113e9082ddb2"}}},"nbformat":4,"nbformat_minor":0}
