{"cells":[{"cell_type":"code","execution_count":163,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17707,"status":"ok","timestamp":1668445295774,"user":{"displayName":"孙菲阳","userId":"00080526783451141965"},"user_tz":-60},"id":"WAi8dgc-J5Jn","outputId":"30dfbcaf-271f-49d4-e455-959ebdbae8f0"},"outputs":[],"source":["%matplotlib inline\n","# from ExKMC.Tree import Tree # import from cloned local library followed by installing manually\n","import sys\n","sys.path.append('../')\n","from ExKMC_M.ExKMC.Tree import Tree\n","\n","from sklearn.datasets import make_blobs\n","import gdown\n","import pandas as pd\n","import copy\n","from sklearn.cluster import KMeans,DBSCAN,BisectingKMeans,MiniBatchKMeans\n","from utils import calc_cost, plot_kmeans, plot_tree_boundary,plot_confusion_matrix\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.preprocessing import StandardScaler, normalize\n","from utils import plot_confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import os\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","from sklearn.utils import check_random_state, check_X_y, _safe_indexing\n","from sklearn.metrics.pairwise import pairwise_distances, pairwise_distances_chunked\n","from sklearn.preprocessing import LabelEncoder\n","import functools\n","from sklearn.feature_selection import SelectKBest, mutual_info_regression,mutual_info_classif\n","\n","from sklearn.decomposition import PCA,SparsePCA,MiniBatchSparsePCA\n"]},{"cell_type":"markdown","metadata":{},"source":["## data preprocessing"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Data shape is (20760, 698)\n"]}],"source":["DF = pd.read_csv('/home/sfy/Documents/VScodeProject/Thesis/family.csv',index_col=False)\n","print(f'Data shape is {DF.shape}')\n","\n","grouped = DF.groupby(['family'])\n","# get numbers of each group \n","groupCount = grouped['family'].count()"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[{"data":{"text/plain":["family\n","Airpush         6652\n","FakeInst        2168\n","Mecor           1820\n","Youmi           1300\n","Fusob           1262\n","Kuguo           1197\n","Dowgin           862\n","BankBot          647\n","Jisut            558\n","DroidKungFu      546\n","RuMMS            402\n","Lotoor           329\n","Mseg             235\n","Boqx             215\n","Minimob          203\n","Triada           197\n","Kyview           175\n","SlemBunk         174\n","SimpleLocker     172\n","SmsKey           165\n","Name: family, dtype: int64"]},"execution_count":200,"metadata":{},"output_type":"execute_result"}],"source":["groupCount.sort_values(ascending=False)[:20]"]},{"cell_type":"code","execution_count":303,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["selected family are ['Airpush', 'FakeInst', 'Mecor', 'Youmi', 'Fusob', 'Kuguo', 'Dowgin', 'BankBot', 'Jisut', 'DroidKungFu']\n","train dataset shape (17012, 698)\n","Without duplications wi shape (17012, 696)\n","With duplications wi shape (17012, 696)\n"]}],"source":["# groupCount\n","# seect the family with certain condition\n","# select by number of instances\n","# selected = groupCount[groupCount > 1000]\n","\n","# select by numbers of the top 5\n","TOP = 10\n","\n","selected = groupCount.sort_values(ascending=False)[:TOP]\n","\n","# get names\n","selectedNames = list(selected.index)\n","print(f\"selected family are {selectedNames}\")\n","\n","# select trained\n","train = DF.loc[DF[\"family\"].isin(selectedNames)]\n","train.reset_index(inplace=True, drop=True)\n","print(f\"train dataset shape {train.shape}\")\n","\n","\n","# encode into encoder\n","le = LabelEncoder()\n","label = le.fit_transform(train[\"family\"])\n","# print(train.head(5))\n","\n","# exclue name -> X_\n","train = train.loc[:, train.columns != \"name\"]\n","train = train.loc[:, train.columns != \"family\"]\n","\n","# remove duplicates -> X\n","# parameter with first and last ? !!!! can be discussed\n","# X = train.drop_duplicates(keep='last')\n","X = train\n","\n","# with X.index canbe reserved to original labels\n","Y = np.take(label, X.index)\n","\n","print(f\"Without duplications wi shape {X.shape}\")\n","print(f\"With duplications wi shape {train.shape}\")\n","\n","X.reset_index(inplace=True, drop=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["## evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### fscore and accuracy"]},{"cell_type":"code","execution_count":304,"metadata":{},"outputs":[{"data":{"text/plain":["4452351.219344967"]},"execution_count":304,"metadata":{},"output_type":"execute_result"}],"source":["# from sklearn.feature_selection import VarianceThreshold,GenericUnivariateSelect,chi2\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=0)\n","k = len(selectedNames)\n","X_train = X\n","X_train.shape\n","\n","# success with variance\n","scaler = StandardScaler()\n","scaled_df = scaler.fit_transform(X_train)\n","\n","# Converting the numpy array into a pandas DataFrame\n","X_train_ = pd.DataFrame(scaled_df)\n","\n","# X_train_ = VarianceThreshold(.8*(1-.8)).fit_transform(X_train_)\n","\n","kmeans = KMeans(k,random_state=0)\n","\n","# kmeans_labels = kmeans.fit_predict(X_train_)\n","# kmeans_labels = kmeans.fit_transform(X_train_)\n","kmeans.fit(X_train_)\n","\n","# tree = Tree(k=k,max_leaves=8 * k)\n","# tree_labels = tree.fit_predict(X_train_, kmeans)\n","\n","# f_a_score(Y, kmeans_labels, tree_labels=tree_labels)\n","kmeans.inertia_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get jaccard\n","# get the sets index of each label kmeans predicted \n","# get the sets index of true label \n","# kmeans.labels_[5000:]\n","\n","for i in range(TOP):\n","    # index of each true\n","    b = np.where(Y == i ) [0]\n","    similarity = list()\n","    for family in range(TOP):\n","        samples = np.where(kmeans.labels_ ==family)[0]\n","        correctly_classified = list(set(samples).intersection(set(b)))\n","        union = list(set(samples).union(set(b)))\n","        jind = len(correctly_classified) / float(len(union))\n","        similarity.append(jind)\n","    \n","    print(f'--Result of family label with {i}')\n","\n","    print(similarity)\n","\n","# get MojoJM result ??????????????????????????"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["result of duplication of all features\n","\n","\n","* no dupli\n","\n","#0-2 61%;1-5 0.01; 2-5 0.24; 3-1 0.07; 4-5; 5-5; 7-5 0.25; 9-0.223\n","#data imblanced\n","\n","* with duplication\n","\n","--Result of family label with 0\n","[0.0002830455703368242, 0.35255452644158947, 0.0014452027298273785, 0.0001503307276007216, 0.0003003003003003003, 0.0063083522583901085, 0.44898612593383136, 0.00015026296018031557, 0.0001503307276007216, 0.0014814814814814814]\n","--Result of family label with 1\n","[0.6353846153846154, 0.0, 0.03753609239653513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","--Result of family label with 2\n","[0.0, 0.0027649769585253456, 0.01750913520097442, 0.0, 0.0057670126874279125, 0.07051909892262488, 0.07689116304798783, 0.0, 0.0, 0.035218783351120594]\n","--Result of family label with 3\n","[0.0, 0.001358695652173913, 0.0009432479169941833, 0.0, 0.0, 0.08974358974358974, 0.05388630272252786, 0.0, 0.0, 0.0]\n","--Result of family label with 4\n","[0.0, 0.0, 0.37244459714825634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","--Result of family label with 5\n","[0.0, 0.000273000273000273, 0.21659223634489866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","--Result of family label with 6\n","[0.0, 0.0, 0.016411727214786488, 0.0, 0.0, 0.0010638297872340426, 0.06443812233285917, 0.0, 0.0, 0.0]\n","--Result of family label with 7\n","[0.0, 0.0025069637883008357, 0.009348482669351359, 0.0, 0.0008291873963515755, 0.1824577861163227, 0.08990874932903918, 0.0, 0.0, 0.050724637681159424]\n","--Result of family label with 8\n","[0.0, 0.0, 0.31266105480158046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","--Result of family label with 9\n","[0.0005830903790087463, 0.005158837903882704, 0.0043723554301833565, 0.0, 0.0015290519877675841, 0.28690534575772436, 0.08681289640591966, 0.0023059185242121443, 0.0, 0.001422475106685633]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[],"source":["# 7: 557883.8453165484 # 6: 388794.13302852295 # ['FakeInst', 'Mecor', 'Youmi', 'Fusob', 'Kuguo'] # 10: 595860.8893492733"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["fsocre of kmenas is 0.07602760060373061\n","fsocre of tree score is 0.07051666035172852\n","accuracy of kmenas is 0.26633794013473155\n","accuracy of tree score is 0.24001666782415446\n"]}],"source":["# calculate the scores with jaccard\n","\n","def f_a_score(true_labels, kmenas_labels, tree_labels):\n","    from sklearn.metrics import f1_score, accuracy_score\n","\n","    print(f\"fsocre of kmenas is {f1_score(y_true=true_labels,y_pred=kmenas_labels,average='macro')}\")\n","    print(f\"fsocre of tree score is {f1_score(y_true=true_labels,y_pred=tree_labels,average='macro')}\")\n","\n","    print(\n","        f\"accuracy of kmenas is {accuracy_score(y_true=true_labels,y_pred=kmenas_labels)}\"\n","    )\n","    print(\n","        f\"accuracy of tree score is {accuracy_score(y_true=true_labels,y_pred=tree_labels)}\"\n","    )\n"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["fsocre of kmenas is 0.5051308711179422\n","fsocre of tree score is 0.5307182751006624\n","accuracy of kmenas is 0.46932641733524005\n","accuracy of tree score is 0.4745618925273287\n"]}],"source":["#random result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def _silhouette_reduce(D_chunk, start, labels, label_freqs):\n","    \"\"\"Accumulate silhouette statistics for vertical chunk of X.\n","\n","    Parameters\n","    ----------\n","    D_chunk : array-like of shape (n_chunk_samples, n_samples)\n","        Precomputed distances for a chunk.\n","    start : int\n","        First index in the chunk.\n","    labels : array-like of shape (n_samples,)\n","        Corresponding cluster labels, encoded as {0, ..., n_clusters-1}.\n","    label_freqs : array-like\n","        Distribution of cluster labels in ``labels``.\n","    \"\"\"\n","    # accumulate distances from each sample to each cluster\n","    clust_dists = np.zeros((len(D_chunk), len(label_freqs)), dtype=D_chunk.dtype)\n","    for i in range(len(D_chunk)):\n","        clust_dists[i] += np.bincount(\n","            labels, weights=D_chunk[i], minlength=len(label_freqs)\n","        )\n","\n","    # intra_index selects intra-cluster distances within clust_dists\n","    intra_index = (np.arange(len(D_chunk)), labels[start : start + len(D_chunk)])\n","    # intra_clust_dists are averaged over cluster size outside this function\n","    intra_clust_dists = clust_dists[intra_index]\n","    # of the remaining distances we normalise and extract the minimum\n","    clust_dists[intra_index] = np.inf\n","    clust_dists /= label_freqs\n","    inter_clust_dists = clust_dists.min(axis=1)\n","    return intra_clust_dists, inter_clust_dists\n","\n","\n","def silhouette_score(\n","    X, labels, *, metric=\"euclidean\", sample_size=None, random_state=None, **kwds\n","):\n","    if sample_size is not None:\n","        X, labels = check_X_y(X, labels, accept_sparse=[\"csc\", \"csr\"])\n","        random_state = check_random_state(random_state)\n","        indices = random_state.permutation(X.shape[0])[:sample_size]\n","        if metric == \"precomputed\":\n","            X, labels = X[indices].T[indices].T, labels[indices]\n","        else:\n","            X, labels = X[indices], labels[indices]\n","    else:\n","        scores = silhouette_samples(X, labels, metric=metric, **kwds)\n","    return np.mean(scores,axis=1)\n","\n","\n","def silhouette_samples(X, labels, *, metric=\"euclidean\", **kwds):\n","    \"\"\"Compute the Silhouette Coefficient for each sample.\n","\n","    The Silhouette Coefficient is a measure of how well samples are clustered\n","    with samples that are similar to themselves. Clustering models with a high\n","    Silhouette Coefficient are said to be dense, where samples in the same\n","    cluster are similar to each other, and well separated, where samples in\n","    different clusters are not very similar to each other.\n","\n","    The Silhouette Coefficient is calculated using the mean intra-cluster\n","    distance (``a``) and the mean nearest-cluster distance (``b``) for each\n","    sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n","    b)``.\n","    Note that Silhouette Coefficient is only defined if number of labels\n","    is 2 ``<= n_labels <= n_samples - 1``.\n","\n","    This function returns the Silhouette Coefficient for each sample.\n","\n","    The best value is 1 and the worst value is -1. Values near 0 indicate\n","    overlapping clusters.\n","    \"\"\"\n","    X, labels = check_X_y(X, labels, accept_sparse=[\"csc\", \"csr\"])\n","\n","    # Check for non-zero diagonal entries in precomputed distance matrix\n","    if metric == \"precomputed\":\n","        error_msg = ValueError(\n","            \"The precomputed distance matrix contains non-zero \"\n","            \"elements on the diagonal. Use np.fill_diagonal(X, 0).\"\n","        )\n","        if X.dtype.kind == \"f\":\n","            atol = np.finfo(X.dtype).eps * 100\n","            if np.any(np.abs(np.diagonal(X)) > atol):\n","                raise ValueError(error_msg)\n","        elif np.any(np.diagonal(X) != 0):  # integral dtype\n","            raise ValueError(error_msg)\n","\n","    le = LabelEncoder()\n","    labels = le.fit_transform(labels)\n","    n_samples = len(labels)\n","    label_freqs = np.bincount(labels)\n","\n","    kwds[\"metric\"] = metric\n","    reduce_func = functools.partial(\n","        _silhouette_reduce, labels=labels, label_freqs=label_freqs\n","    )\n","    results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func, **kwds))\n","    intra_clust_dists, inter_clust_dists = results\n","    # intra:35655.59,inter:1.559\n","    intra_clust_dists = np.concatenate(intra_clust_dists)\n","    inter_clust_dists = np.concatenate(inter_clust_dists)\n","# unknow operatino of denom # after denom the dists become 1.49 before is 30234\n","    denom = (label_freqs - 1).take(labels, mode=\"clip\")\n","    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","        intra_clust_dists /= denom\n","\n","    sil_samples = inter_clust_dists - intra_clust_dists\n","    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","        sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)\n","    # nan values are for clusters of size 1, and should be 0\n","    return [intra_clust_dists,inter_clust_dists,np.nan_to_num(sil_samples)]\n","\n","\n","# print(f'intra and inter distance of kmenas is {silhouette_score(X=X_train, labels=kmeans_labels)}')\n","# print(f'intra and inter distance of tree score is {silhouette_score(X=X_train, labels=tree_labels)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cost\n","# kmeas cost in paper: The k-means cost is the sum of squared distances of each point to the mean of points associated with the cluster.\n","# kmenas cost in sklearn:Opposite of the value of X on the K-means objective.\n","# surrogate cost:The k-means surrogate cost is the sum of squared distances of each point to the closest center of the kmeans given (or trained) in the fit method.k-means surrogate cost > k-means cost, as k-means cost is computed with respect to the optimal centers.\n","\n","kmeas_cost = tree.score(X_train)\n","surrogate_score = tree.surrogate_score(X_train)\n","print(\n","    f\"kmeans_cost is {kmeas_cost} \\nsurrogate_score is {surrogate_score}\\nkmenas cost in surrogate is {kmeans.score(X_train)}\"\n",")\n","\n","#test with distancs munally\n","# inter-distance\n","from scipy.spatial.distance import euclidean\n","\n","dst = euclidean(kmeans.cluster_centers_[0], kmeans.cluster_centers_[1])\n","print(f\"inter distance from kmeans:{dst}\")\n","\n","tree_centers = tree.get_centers()\n","tree_dst = euclidean(tree_centers[0], tree_centers[1])\n","print(f\"inter distance from tree:{tree_dst}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNrV8z86W1ElNSu4E21X3YW","provenance":[]},"kernelspec":{"display_name":"thesis","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"c673639e1878fb3520d597d27aecf39bb774213fc27bbc445709cd358b48682f"}}},"nbformat":4,"nbformat_minor":0}
